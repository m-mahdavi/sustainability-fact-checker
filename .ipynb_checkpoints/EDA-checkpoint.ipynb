{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483fa58d-8799-401e-b467-b2df452b0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb8650e-8ad6-4da7-b720-2e11e27e9ca0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Upload images\u001b[39;00m\n\u001b[1;32m      3\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "image_paths = list(uploaded.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63537c13-f152-4ea0-9957-adda7bf1de77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-qkfhcz9j\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-qkfhcz9j\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git transformers pillow torch torchvision pandas sentence-transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11bd32-71f5-4a46-96c8-9c3db18bbf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from google.colab import files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca24ff-30d9-4821-87ee-2db612a4e51a",
   "metadata": {},
   "source": [
    "## Creating the dataset's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f4c57-7715-4a80-ba83-6cf5462c7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "google_goals = ['Educate employees across all levels',\n",
    "            'Reward progress and integrate recognition in employee performance assessment',\n",
    "            'Help develop a Forest Positive approach for the forest products industry that is based on sound science and delivers forest health benefits. As the concept of a Forest Positive approach is better defined, we will seek to implement Forest Positive actions that we believe will serve to sustain and expand the protection of working forests P&G depends on']\n",
    "\n",
    "goal_tokens_clip = [clip.tokenize([goal]).to(device) for goal in google_goals]\n",
    "goal_embeddings_sbert = sbert_model.encode(google_goals)\n",
    "\n",
    "column_names = [\"Image Name\", \"BLIP Caption\"] + \\\n",
    "               [f\"Goal_{i+1}\" for i in range(len(google_goals))] + \\\n",
    "               [f\"TF-IDF_Goal_{i+1}\" for i in range(len(google_goals))] + \\\n",
    "               [f\"SBERT_Goal_{i+1}\" for i in range(len(google_goals))] + \\\n",
    "               [f\"Image-Goal_{i+1}\" for i in range(len(google_goals))]\n",
    "\n",
    "results = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        blip_inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            blip_output = blip_model.generate(**blip_inputs)\n",
    "            blip_caption = blip_processor.decode(blip_output[0], skip_special_tokens=True)\n",
    "\n",
    "        image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(image_input)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        caption_tokenized = clip.tokenize([blip_caption]).to(device)\n",
    "        with torch.no_grad():\n",
    "            caption_features = clip_model.encode_text(caption_tokenized)\n",
    "        caption_features = caption_features / caption_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([blip_caption] + google_goals)\n",
    "        tfidf_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "        caption_embedding_sbert = sbert_model.encode([blip_caption])\n",
    "        sbert_similarities = cosine_similarity(caption_embedding_sbert, goal_embeddings_sbert).flatten()\n",
    "\n",
    "        image_goal_similarities = []\n",
    "        for goal_token in goal_tokens_clip:\n",
    "            with torch.no_grad():\n",
    "                goal_features = clip_model.encode_text(goal_token)\n",
    "            goal_features = goal_features / goal_features.norm(dim=-1, keepdim=True)\n",
    "            image_goal_similarity = (image_features @ goal_features.T).item()\n",
    "            image_goal_similarities.append(image_goal_similarity)\n",
    "\n",
    "        row_data = {\n",
    "            \"Image Name\": image_path,\n",
    "            \"BLIP Caption\": blip_caption,\n",
    "        }\n",
    "        row_data.update({f\"Goal_{i+1}\": google_goals[i] for i in range(len(google_goals))})\n",
    "        row_data.update({f\"TF-IDF_Goal_{i+1}\": tfidf_similarities[i] for i in range(len(tfidf_similarities))})\n",
    "        row_data.update({f\"SBERT_Goal_{i+1}\": sbert_similarities[i] for i in range(len(sbert_similarities))})\n",
    "        row_data.update({f\"Image-Goal_{i+1}\": image_goal_similarities[i] for i in range(len(image_goal_similarities))})\n",
    "\n",
    "        results.append(row_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "dataset = pd.DataFrame(results, columns=column_names)\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab888bca-3dbd-4ef4-85b0-364eb1b5c83e",
   "metadata": {},
   "source": [
    "## extracted goals from the pivot goal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1460d4f4-befd-4d0a-967f-28c414917494",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinal_focus = ['Climate', 'GHG', 'Governance', 'Distribution', 'Compliance Standards', 'Transparency, Waste']\n",
    "cardinal_goals = ['Minimize waste generated in our operations and maintain high rates of reuse and recycling' ,\n",
    "                 'By the end of 2021, we plan to set a public greenhouse gas emissions reduction goal for our pharmaceutical distribution business']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3d21961-d7f1-4198-ae06-7cf3b530afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "boeing_focus = ['Toxics', 'Compliance Standards', 'Transparency','Renewables', 'Fuel']\n",
    "boeing_goals = ['Catalyze the industry toward sustainable aviation fuel commercialization',\n",
    "               'Work with aviation stakeholders to ensure that all Boeing products comply with chemical restrictions and bans ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efc83731-ce85-411d-b236-64bf0c37391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_focus = ['Governance', 'Compliance Standards', 'Transparency']\n",
    "citi_goals = ['Update Statement of Supplier Principles and disseminate it to all suppliers.',\n",
    "             'Monitor emerging risks and trends',\n",
    "             'Achieve 100% of suppliers to respond to Corporate Responsibility Questionnaire (CRQ) in all regions by 2019',\n",
    "             'Finalize and implement supply chain policies for paper and paper products, IT and e-waste disposal and travel and logistics by 2019.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff159aed-4ab1-4685-92d1-3119fde788b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_focus = ['Energy', 'Buildings','Climate','GHG','Water']\n",
    "google_goals = ['Maintain or improve quarterly PUE at each Google data center, year over year', \n",
    "               'Maintain carbon neutrality for our operations each year',\n",
    "               'Reduce potable water intensity at our Bay Area headquarters by 5% by the end of 2019, against a 2017 baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21e381b0-735c-45d2-bc91-0100bbec789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "khc_focus = ['Energy', 'Water','Land Use', 'Forest', 'Food & Ag', 'Transparency']\n",
    "khc_goals = ['Reduce energy consumption by 15% by 2020 (2015 baseline)', \n",
    "             'Reduce water consumption by 15% by 2020 (2015 baseline)',\n",
    "             'Purchase 100% sustainable and traceable palm oil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe73e521-03ae-4580-8760-10c00bc6ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfizer_focus = ['Climate', 'GHG', 'Water' ]\n",
    "pfizer_goals = ['By 2020, total hazardous and nonhazardous waste will be reduced by 15% compared with the 2012 baseline',\n",
    "                'By 2020, greenhouse gas emissions (Total Scope 1 and Scope 2) will be reduced by 20% compared with the 2012 baseline',\n",
    "                'By 2020, total water withdrawal (excluding non-contact cooling water) will be reduced by 5% compared with the 2012 baseline.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78c865d2-69b0-4b51-9f98-dc754b54ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "PG_focus = ['Forest','ComplianceStandards']\n",
    "PG_goals = ['Educate employees across all levels',\n",
    "            'Reward progress and integrate recognition in employee performance assessment',\n",
    "            'Help develop a Forest Positive approach for the forest products industry that is based on sound science and delivers forest health benefits. As the concept of a Forest Positive approach is better defined, we will seek to implement Forest Positive actions that we believe will serve to sustain and expand the protection of working forests P&G depends on']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9f217-760f-4707-8fde-5e6670ca9e74",
   "metadata": {},
   "source": [
    "## labeling part code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc7c0a7e-c426-40d1-861d-0e0194601f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeling complete! The updated dataset has been saved to ./datasets/V2/labeled_p&g.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output \n",
    "\n",
    "google_goals = ['Educate employees across all levels',\n",
    "            'Reward progress and integrate recognition in employee performance assessment',\n",
    "            'Help develop a Forest Positive approach for the forest products industry that is based on sound science and delivers forest health benefits. As the concept of a Forest Positive approach is better defined, we will seek to implement Forest Positive actions that we believe will serve to sustain and expand the protection of working forests P&G depends on']\n",
    "\n",
    "dataset_path = './datasets/V2/p&g.csv'\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "if 'Image Name' not in df.columns:\n",
    "    raise ValueError(\"Dataset must have a column named 'Image Name'\")\n",
    "\n",
    "image_folder = './Pictures/P&G/'\n",
    "\n",
    "labels_data = []\n",
    "\n",
    "def clear_screen():\n",
    "    if os.name == 'nt':\n",
    "        os.system('cls')\n",
    "    else:\n",
    "        os.system('clear')\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    image_name = row['Image Name']\n",
    "    image_path = os.path.join(image_folder, image_name)\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Warning: Image {image_name} not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  \n",
    "    plt.draw() \n",
    "    plt.pause(0.1)  \n",
    "    plt.close()  \n",
    "\n",
    "    print(f\"\\nLabeling Image: {image_name}\")\n",
    "    for i, goal in enumerate(google_goals, start=1):\n",
    "        print(f\"\\nGoal {i}: {goal}\")\n",
    "        label = input(f\"Is this image relevant to Goal {i}? (1 for Yes, 0 for No): \")\n",
    "\n",
    "        while label not in ['0', '1']:\n",
    "            print(\"Invalid input. Please enter 1 (Yes) or 0 (No).\")\n",
    "            label = input(f\"Is this image relevant to Goal {i}? (1 for Yes, 0 for No): \")\n",
    "\n",
    "        labels_data.append((image_name, f\"Goal {i} Label\", label))\n",
    "\n",
    "    if os.environ.get('DISPLAY', '') == '':  # Terminal-based environments\n",
    "        clear_screen()\n",
    "    else:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "labels_df = pd.DataFrame(labels_data, columns=['Image Name', 'Goal', 'Label'])\n",
    "\n",
    "labels_df_pivot = labels_df.pivot(index='Image Name', columns='Goal', values='Label')\n",
    "\n",
    "df = pd.merge(df, labels_df_pivot, on='Image Name', how='left')\n",
    "\n",
    "output_path = './datasets/V2/labeled_p&g.csv'  # Specify the output path\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nLabeling complete! The updated dataset has been saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b529e6a-cdab-4d25-a8ad-ca26ebd2addf",
   "metadata": {},
   "source": [
    "## result aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cdb1968-cbee-40b0-85c2-21800968ce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP results saved to './datasets/V2/map_results_for_top_10.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def calculate_map(relevance_list):\n",
    "    total_relevant = sum(relevance_list)\n",
    "    if total_relevant == 0:\n",
    "        return 0.0  \n",
    "    \n",
    "    precision_sum = 0.0\n",
    "    relevant_count = 0\n",
    "    \n",
    "    for idx, relevance in enumerate(relevance_list, start=1):\n",
    "        if relevance == 1:\n",
    "            relevant_count += 1\n",
    "            precision_at_k = relevant_count / idx\n",
    "            precision_sum += precision_at_k\n",
    "    \n",
    "    ap = precision_sum / total_relevant\n",
    "    return ap\n",
    "\n",
    "def compute_map_for_datasets(file_paths, top_k=10):\n",
    "    results = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        dataset_name = os.path.basename(file_path).split('.')[0]\n",
    "        data = pd.read_csv(file_path)\n",
    "                methods = [\"TF-IDF\", \"SBERT\", \"Image\"]\n",
    "        \n",
    "        for method in methods:\n",
    "            for col in data.columns:\n",
    "                if method in col:\n",
    "                    goal_number = col.split(\"_\")[-1]  # e.g., Goal_1, Goal_2, etc.\n",
    "                    goal_label_col = f\"Goal {goal_number} Label\"  # Corresponding goal label column\n",
    "                    \n",
    "                    if goal_label_col in data.columns:\n",
    "                        sorted_data = data.sort_values(by=col, ascending=False)\n",
    "                        \n",
    "                        top_k_relevance = sorted_data[goal_label_col].head(top_k).tolist()\n",
    "                        \n",
    "                        map_score = calculate_map(top_k_relevance)\n",
    "                                                results.append({\n",
    "                            \"Dataset\": dataset_name,\n",
    "                            \"Goal\": goal_number,\n",
    "                            \"Method\": method,\n",
    "                            \"MAP\": map_score\n",
    "                        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "file_paths = [\n",
    "    './datasets/V2/labeled_boeing.csv',\n",
    "    './datasets/V2/labeled_cardinal.csv',\n",
    "    './datasets/V2/labeled_citi.csv',\n",
    "    './datasets/V2/labeled_google.csv',\n",
    "    './datasets/V2/labeled_khc.csv',\n",
    "    './datasets/V2/labeled_pfizer.csv',\n",
    "    './datasets/V2/labeled_p&g.csv'\n",
    "]\n",
    "\n",
    "top_k = 10  \n",
    "map_results = compute_map_for_datasets(file_paths, top_k=top_k)\n",
    "\n",
    "output_file = './datasets/V2/map_results_for_top_10.csv'\n",
    "map_results.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "421bdf25-38f7-450f-9f3a-d94524653a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method statistics (mean, std) saved to './datasets/V2/method_stats.csv'.\n",
      "   Method      mean       std\n",
      "0   Image  0.464315  0.369896\n",
      "1   SBERT  0.384954  0.394615\n",
      "2  TF-IDF  0.213894  0.289296\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "map_results = pd.read_csv('./datasets/V2/map_results_for_top_10.csv')\n",
    "\n",
    "method_stats = map_results.groupby(\"Method\")[\"MAP\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "\n",
    "method_stats_file = './datasets/V2/method_stats.csv'\n",
    "method_stats.to_csv(method_stats_file, index=False)\n",
    "\n",
    "print(f\"Method statistics (mean, std) saved to '{method_stats_file}'.\")\n",
    "print(method_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c49e34-3601-4c8f-b9fc-1e4c9c6015b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
